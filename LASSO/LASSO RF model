from collections import Counter
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import roc_curve, roc_auc_score

def get_lasso_min_features_cv(X, y, feature_names, repeats=10, cv=5, save_path=None):
    selected = []
    for seed in range(repeats):
        lasso = LogisticRegressionCV(
            Cs=[0.01, 0.1, 1],
            cv=cv,
            penalty='l1',
            solver='liblinear',
            scoring='roc_auc',
            max_iter=10000,
            random_state=seed
        )
        lasso.fit(X, y)
        coef = lasso.coef_.flatten()
        selected += list(feature_names[np.where(coef != 0)])

    counts = Counter(selected)
    sorted_items = counts.most_common()
    selected_features = set([f for f, c in counts.items() if c >= int(repeats * 1)])
    coef = lasso.coef_.flatten()
    nonzero_mask = coef != 0
    selected_names = feature_names[nonzero_mask]
    selected_coefs = coef[nonzero_mask]
    coef_dict = dict(zip(selected_names, selected_coefs))

    if len(coef_dict) > 0:
        sorted_items = sorted(coef_dict.items(), key=lambda x: abs(x[1]))
        sorted_names = [k for k, v in sorted_items]
        sorted_coefs = [v for k, v in sorted_items]

        plt.figure(figsize=(10, 5))
        plt.barh(range(len(sorted_coefs)), np.abs(sorted_coefs), color='darkblue')
        plt.yticks(range(len(sorted_coefs)), sorted_names)
        plt.xlabel("Absolute LASSO Coefficient", fontsize=14)
        plt.title("LASSO Coefficients of Features", fontsize=16)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path.replace('.png', '_coef.png'), dpi=300)
        else:
            plt.show()

    return selected_features, coef_dict



def get_rf_top_features_cv(X, y, feature_names, top_n=10, n_splits=10):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    importances_accum = np.zeros(X.shape[1])

    for train_idx, _ in skf.split(X, y):
        X_train, y_train = X[train_idx], y[train_idx]

        rf = RandomForestClassifier(
            n_estimators=500,
            max_depth=8,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        rf.fit(X_train, y_train)
        importances_accum += rf.feature_importances_

    importances_mean = importances_accum / n_splits
    top_indices = np.argsort(importances_mean)[::-1][:top_n]
    top_features = feature_names[top_indices]

    sorted_idx = np.argsort(importances_mean)[::-1]
    sorted_features = feature_names[sorted_idx]
    sorted_importances = importances_mean[sorted_idx]

    plt.figure(figsize=(10, 6))
    plt.scatter(sorted_features, sorted_importances, color='darkblue', s=50, alpha=0.7)

    plt.xlabel("Feature", fontsize=14)
    plt.ylabel("Importance Score", fontsize=14)
    plt.title("Random Forest Feature Importance", fontsize=16)
    plt.xticks(rotation=90, fontsize=10)
    plt.yticks(fontsize=12)
    plt.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.6)

    ax = plt.gca()
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_linewidth(1.2)
    ax.spines['left'].set_linewidth(1.2)

    plt.tight_layout()
    plt.show()

    return set(top_features)




def evaluate_model_lasso(X_train, y_train, X_test, y_test, selected_features, feature_names, pos_label):
    indices = [np.where(feature_names == f)[0][0] for f in selected_features]
    X_train_sel = X_train[:, indices]
    X_test_sel = X_test[:, indices]

    clf = LogisticRegressionCV(
        Cs=np.logspace(-3, 2, 20), 
        cv=3,
        penalty='l1',
        solver='saga',
        scoring='roc_auc',
        max_iter=10000,
        random_state=0
    )
    clf.fit(X_train_sel, y_train)
    y_prob = clf.predict_proba(X_test_sel)[:, 1]

    fpr, tpr, thresholds = roc_curve(y_test == pos_label, y_prob)
    youden_index = tpr - fpr
    best_threshold = thresholds[np.argmax(youden_index)]

    y_pred = (y_prob >= best_threshold).astype(int)
    cm = confusion_matrix(y_test == pos_label, y_pred)
    auc = roc_auc_score(y_test == pos_label, y_prob)
    sens = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0
    spec = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0
    return auc, sens, spec, best_threshold



def bootstrap_evaluate_model_lasso(
    X_train, y_train, X_test, y_test, selected_features, feature_names,
    pos_label, n_bootstrap=5000, plot_roc=True
):
    auc_list, sens_list, spec_list = [], [], []

    for i in range(n_bootstrap):
        X_test_boot, y_test_boot = resample(X_test, y_test, replace=True, stratify=y_test, random_state=i)

        try:
            auc, sens, spec, _ = evaluate_model_lasso(
                X_train, y_train, X_test_boot, y_test_boot,
                selected_features, feature_names, pos_label
            )
            auc_list.append(auc)
            sens_list.append(sens)
            spec_list.append(spec)
        except Exception:
            continue

    def summary_stats(lst):
        return np.mean(lst), np.percentile(lst, 2.5), np.percentile(lst, 97.5)

    auc_mean, auc_ci_low, auc_ci_high = summary_stats(auc_list)
    sens_mean, sens_ci_low, sens_ci_high = summary_stats(sens_list)
    spec_mean, spec_ci_low, spec_ci_high = summary_stats(spec_list)

    print(f"AUC: {auc_mean:.3f} (95% CI: {auc_ci_low:.3f}–{auc_ci_high:.3f})")
    print(f"Sensitivity: {sens_mean:.3f} (95% CI: {sens_ci_low:.3f}–{sens_ci_high:.3f})")
    print(f"Specificity: {spec_mean:.3f} (95% CI: {spec_ci_low:.3f}–{spec_ci_high:.3f})")

    if plot_roc:
        indices = [np.where(feature_names == f)[0][0] for f in selected_features]
        X_train_sel = X_train[:, indices]
        X_test_sel = X_test[:, indices]

        clf = LogisticRegressionCV(
            Cs=np.logspace(-3, 2, 20),  
            cv=3,
            penalty='l1',
            solver='saga',
            scoring='roc_auc',
            max_iter=10000,
            random_state=0
        )
        clf.fit(X_train_sel, y_train)
        y_prob = clf.predict_proba(X_test_sel)[:, 1]
        fpr, tpr, _ = roc_curve(y_test == pos_label, y_prob)
        auc_score = roc_auc_score(y_test == pos_label, y_prob)

        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, color='darkorange', lw=2)
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.tight_layout()
        textstr = '\n'.join((
            f'AUC = {auc_mean:.3f} (95% CI: {auc_ci_low:.3f}–{auc_ci_high:.3f})',
            f'Sensitivity = {sens_mean:.3f} (95% CI: {sens_ci_low:.3f}–{sens_ci_high:.3f})',
            f'Specificity = {spec_mean:.3f} (95% CI: {spec_ci_low:.3f}–{spec_ci_high:.3f})'
        ))

        plt.gca().text(0.4, 0.2, textstr, transform=plt.gca().transAxes, fontsize=10,
                       verticalalignment='top',
                       bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.5'))
        plt.show()

    return {
        "auc": (auc_mean, auc_ci_low, auc_ci_high),
        "sensitivity": (sens_mean, sens_ci_low, sens_ci_high),
        "specificity": (spec_mean, spec_ci_low, spec_ci_high)
    }



def bootstrap_evaluate_model_lasso(
    X_train, y_train, X_test, y_test, selected_features, feature_names,
    pos_label, n_bootstrap=5000, plot_roc=True
):
    auc_list, sens_list, spec_list = [], [], []

    for i in range(n_bootstrap):
        X_test_boot, y_test_boot = resample(
            X_test, y_test, replace=True, stratify=y_test, random_state=i
        )

        try:
            auc, sens, spec, _ = evaluate_model_lasso(
                X_train, y_train, X_test_boot, y_test_boot,
                selected_features, feature_names, pos_label
            )
            auc_list.append(auc)
            sens_list.append(sens)
            spec_list.append(spec)
        except Exception:
            continue

    def summary_stats(lst):
        return np.mean(lst), np.percentile(lst, 2.5), np.percentile(lst, 97.5)

    auc_mean, auc_ci_low, auc_ci_high = summary_stats(auc_list)
    sens_mean, sens_ci_low, sens_ci_high = summary_stats(sens_list)
    spec_mean, spec_ci_low, spec_ci_high = summary_stats(spec_list)

    print(f"AUC: {auc_mean:.3f} (95% CI: {auc_ci_low:.3f}–{auc_ci_high:.3f})")
    print(f"Sensitivity: {sens_mean:.3f} (95% CI: {sens_ci_low:.3f}–{sens_ci_high:.3f})")
    print(f"Specificity: {spec_mean:.3f} (95% CI: {spec_ci_low:.3f}–{spec_ci_high:.3f})")

    if plot_roc:
        indices = [np.where(feature_names == f)[0][0] for f in selected_features]
        X_train_sel = X_train[:, indices]
        X_test_sel = X_test[:, indices]

        clf = LogisticRegressionCV(
            Cs=np.logspace(-3, 2, 20),
            cv=3,
            penalty='l1',
            solver='saga',
            scoring='roc_auc',
            max_iter=10000,
            random_state=0
        )
        clf.fit(X_train_sel, y_train)
        y_prob = clf.predict_proba(X_test_sel)[:, 1]
        fpr, tpr, _ = roc_curve(y_test == pos_label, y_prob)
        auc_score = roc_auc_score(y_test == pos_label, y_prob)

        ax.plot(fpr, tpr, color='darkorange', lw=2)
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

        ax.set_xlabel("False Positive Rate")
        ax.set_ylabel("True Positive Rate")

        textstr = '\n'.join((
            f'AUC = {auc_mean:.3f} (95% CI: {auc_ci_low:.3f}–{auc_ci_high:.3f})',
            f'Sensitivity = {sens_mean:.3f} (95% CI: {sens_ci_low:.3f}–{sens_ci_high:.3f})',
            f'Specificity = {spec_mean:.3f} (95% CI: {spec_ci_low:.3f}–{spec_ci_high:.3f})'
        ))

        ax.text(
            0.4, 0.2, textstr,
            transform=ax.transAxes,
            fontsize=10,
            verticalalignment='top',
            bbox=dict(
                facecolor='white',  
                alpha=0.7,
                edgecolor='none',
                boxstyle='round,pad=0.5'
             )
        )

        plt.tight_layout()
        plt.show()


    return {
        "auc": (auc_mean, auc_ci_low, auc_ci_high),
        "sensitivity": (sens_mean, sens_ci_low, sens_ci_high),
        "specificity": (spec_mean, spec_ci_low, spec_ci_high)
    }


from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import numpy as np

def get_rf_top_features_cv_with_gridsearch(X, y, feature_names, top_n=10, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    importances_accum = np.zeros(X.shape[1])

    param_grid = {
        'n_estimators': [200, 300, 500],
        'max_depth': [5, 8, 12],
        'max_features': ['sqrt', 'log2']
    }

    for train_idx, _ in skf.split(X, y):
        X_train, y_train = X[train_idx], y[train_idx]

        rf = RandomForestClassifier(random_state=42)
        grid_search = GridSearchCV(
            estimator=rf,
            param_grid=param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=0
        )
        grid_search.fit(X_train, y_train)

        best_rf = grid_search.best_estimator_
        importances_accum += best_rf.feature_importances_

    importances_mean = importances_accum / n_splits
    top_indices = np.argsort(importances_mean)[::-1][:top_n]
    top_features = feature_names[top_indices]

    sorted_idx = np.argsort(importances_mean)[::-1]
    sorted_features = feature_names[sorted_idx]
    sorted_importances = importances_mean[sorted_idx]

    plt.figure(figsize=(10, 6))
    plt.scatter(sorted_features, sorted_importances, color='darkblue', s=50, alpha=0.7)
    plt.xlabel("Feature", fontsize=14)
    plt.ylabel("Importance Score", fontsize=14)
    plt.title("Random Forest Feature Importance (with Grid Search)", fontsize=16)
    plt.xticks(rotation=90, fontsize=10)
    plt.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.6)

    ax = plt.gca()
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_linewidth(1.2)
    ax.spines['left'].set_linewidth(1.2)

    plt.tight_layout()
    plt.show()
    print("Top 10 features and their average importance:")
    for i in top_indices:
        print(f"{feature_names[i]}: {importances_mean[i]:.4f}")

    return set(top_features)




df = pd.read_csv("")
X = df.iloc[:, 1:].values
y = df.iloc[:, 0].values
feature_names = np.array(df.columns[1:])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=100)

def binary_subset(X, y, g1, g2):
    mask = (y == g1) | (y == g2)
    return X[mask], y[mask]

X_train_ac, y_train_ac = binary_subset(X_train, y_train, 1, 3)
X_test_ac, y_test_ac = binary_subset(X_test, y_test, 1, 3)
y_train_ac = (y_train_ac == 3).astype(int)
y_test_ac = (y_test_ac == 3).astype(int)

X_train_bc, y_train_bc = binary_subset(X_train, y_train, 2, 3)
X_test_bc, y_test_bc = binary_subset(X_test, y_test, 2, 3)
y_train_bc = (y_train_bc == 3).astype(int)
y_test_bc = (y_test_bc == 3).astype(int)

lasso_ac, coef_dict = get_lasso_min_features_cv(
    X=X_train_ac,
    y=y_train_ac,
    feature_names=np.array(df.columns[1:]),
    repeats=10,
    save_path="lasso_result.png"
)

rf_ac = get_rf_top_features_cv(X_train_ac, y_train_ac, feature_names, top_n=10, n_splits=5)
ac_features = lasso_ac.union(rf_ac)

lasso_bc, coef_dict = get_lasso_min_features_cv(
    X=X_train_bc,
    y=y_train_bc,
    feature_names=np.array(df.columns[1:]),
    repeats=10,
    save_path="lasso_result.png"
)
rf_bc = get_rf_top_features_cv(X_train_bc, y_train_bc, feature_names, top_n=10, n_splits=5)
bc_features = lasso_bc.union(rf_bc)


print("A vs C — LASSO特征：", lasso_ac)
print("A vs C — RF特征：", rf_ac)
print("B vs C — LASSO特征：", lasso_bc)
print("B vs C — RF特征：", rf_bc)


all_selected = list(lasso_ac) + list(rf_ac) + list(lasso_bc) + list(rf_bc)

feature_counter = Counter(all_selected)

common_features = {f for f, count in feature_counter.items() if count >= 3}

print("出现 ≥3 次的稳定特征集合：", common_features)

import pandas as pd

methods = ['LASSO_AC', 'RF_AC', 'LASSO_BC', 'RF_BC']
feature_list = sorted(set(all_selected))
data = []

for f in feature_list:
    row = [
        int(f in lasso_ac),
        int(f in rf_ac),
        int(f in lasso_bc),
        int(f in rf_bc)
    ]
    data.append(row)

df_bin = pd.DataFrame(data, index=feature_list, columns=methods)

import matplotlib.pyplot as plt

sorted_items = sorted(feature_counter.items(), key=lambda x: x[1], reverse=True)
features, counts = zip(*sorted_items)

plt.figure(figsize=(10, 6))
bars = plt.barh(range(len(counts)), counts[::-1])
plt.yticks(range(len(features)), features[::-1])
plt.xlabel("Feature Occurrence Count")
plt.title("Feature Selection Frequency Across Four Models")

for i, bar in enumerate(bars):
    if counts[::-1][i] >= 3:
        bar.set_color("orange")

plt.axvline(x=3, color='gray', linestyle='--', label="Threshold: ≥3 times")
plt.xticks(range(1, max(counts) + 1, 1))  
plt.legend()
plt.tight_layout()
plt.show()


results = bootstrap_evaluate_model_lasso(
    X_train_ac, y_train_ac,
    X_test_ac, y_test_ac,
    common_features,
    feature_names,
    pos_label=1,
    n_bootstrap=1000
)

results = bootstrap_evaluate_model_lasso(
    X_train_bc, y_train_bc,
    X_test_bc, y_test_bc,
    common_features,
    feature_names,
    pos_label=1,
    n_bootstrap=1000
)
